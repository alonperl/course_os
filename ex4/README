griffonn, ednussi
Gregory Pasternak (327148417), Eran Nussinovitch (302186408)
Ex: 4

FILES:
README
Makefile
CachingFileSystem.cpp
CacheData.hpp
CacheData.cpp
DataBlock.hpp
DataBlock.cpp

REMARKS: NONE

DESIGN DECISIONS:
Apart from the caching_* function, that are at most straightforward, we have:
-   a DataBlock class for single data block that holds the data itself, path of 
    the file it is taken from, absolute block number in that file (from 0), and 
    access count.
-   a CacheData class for managing insertion and removal to/from cache, calculating
    paths, and also holds the cache itself, as far as basic settings of current
    run.
The cache is basically a sorted set of blocks ordered by access count in order to
give us ability to remove least frequently used blocks instantly (O(1)) when needed.
In addition, in order to find blocks that are saved in cache, we have an unordered 
map, which values are of type DataBlock, and keys are in format
"<full_path_to_file>:<absolute_block_number>", so that knowing what block we want and
from what file we can instantly say if it is in cache or not.
Absolute block number is calculated by taking file_size modulo block_size.

ANSWERS TO QUESTIONS:

1.
Yes, as far as cache is in heap (in RAM memory and not on disk), access to it is
much faster, assuming that the FUSE process doesn't go in swap by OS decisions
(in that case we will need to wait for OS to bring it back from the disk and it 
will take the same time as without cache).

2.
Let's talk about both.
	Array-based: 
	Preferrable when the cache state is relatively stable (i.e. the blocks are 
	changed seldom).
	Memory for all the array (i.e. (block_size * block_number) of bytes)
have to be continuous and to be allocated from the beginning; knowing what cell to go 
to, access time is constant; however searching for least frequently used block is 
linear (iterate over every cell). Moreover, when initializing such an array, it can be 
ploblematic to get all the memory if the cache size is big.

	Sorted list-based:
	Preferrable when the blocks are removed frequently, and new blocks are added.
	The large advantage is that pushing new block is logarithmic in time,
and finding the candidate for removal is constant (simply take the head). Also, it does
not allocate for itself memory that is currently unneeded (depends on implemenation).
However, it can be splitted into different pages of the memory, not necessarily continuous,
so it can take more time to find specific block.


3.
It will be much harder to manage LRU for swapping pages since the hardware is the one
which manages the memory access, while the Operating system is the one which manages 
the filesystem access. Clock-cycle based algorithms require less memory, while more
sophisticated memory access algorithms will be very diffcult to implement in the hardware.

4. 
LRU is better when:
A file "foo" is used many times in a small period of time. After that the file is 
no longer used, while other files are used in a moderate manner. In case of LFU 
"foo" would stay in the cache forever, just using up space. But LRU will free it up
whenever it needs more space in cache.

LFU is better when:
If we use on a regular base X files in the same frequency and an additional file 
is used 10 times as much. LRU would free this file from the cache, while LFU will
understand the file is probably will be used more soon.

Both are no good:
If we use X files in a particular order each time, one always follow after the 
other, neither LFU nor LRU would be good for this case and would recognize the specil attribute.

5.
The ideal block-size in this execise would be the size of the real filesystem page.
When we use a smaller value, we will need to use many cache blocks for each real block (and it
can probably not fit exactly on real disk or memory page and an unneeded overhead will come).
When we use a bigger value, we will have unused space.